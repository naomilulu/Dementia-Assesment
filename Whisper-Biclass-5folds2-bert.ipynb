{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "approximate-morris",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'export' ���O�����Υ~���R�O�B�i���檺�{���Χ妸�ɡC\n"
     ]
    }
   ],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "mighty-helping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "# os.environ['CUDA_VISIBLE_DEVICES']='0,1'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']='1'\n",
    "import torch\n",
    "# torch.cuda.set_device(0)\n",
    "print(torch.cuda.current_device())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "smaller-isolation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "narrow-laser",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "neural-absolute",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "maritime-president",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cudaSetDevice(0)\n",
    "#dataset transcript要在dataset資好夾調\n",
    "#cuda 在basesolver init調\n",
    "#dataset要在data.py設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "graduate-business",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "\n",
    "# import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "# # os.environ['CUDA_VISIBLE_DEVICES']='0,1'\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING']='1'\n",
    "\n",
    "# torch.cuda.set_device(0)\n",
    "# print(torch.cuda.current_device())\n",
    "\n",
    "import yaml\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "class Para(object):\n",
    "    a=1\n",
    "\n",
    "def force_cudnn_initialization():\n",
    "    s = 32\n",
    "    dev = torch.device('cuda:0')\n",
    "    torch.nn.functional.conv2d(torch.zeros(s, s, s, s, device=dev), torch.zeros(s, s, s, s, device=dev))\n",
    "    \n",
    "#force_cudnn_initialization()\n",
    "def main():\n",
    "    # For reproducibility, comment these may speed up training\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Arguments\n",
    "#     parser = argparse.ArgumentParser(description='Training E2E asr.')\n",
    "#     parser.add_argument('--config', type=str, help='Path to experiment config.')\n",
    "#     parser.add_argument('--name', default=None, type=str, help='Name for logging.')\n",
    "#     parser.add_argument('--logdir', default='log/', type=str,\n",
    "#                         help='Logging path.', required=False)\n",
    "#     parser.add_argument('--ckpdir', default='ckpt/', type=str,\n",
    "#                         help='Checkpoint path.', required=False)\n",
    "#     parser.add_argument('--outdir', default='result/', type=str,\n",
    "#                         help='Decode output path.', required=False)\n",
    "#     parser.add_argument('--load', default=None, type=str,\n",
    "#                         help='Load pre-trained model (for training only)', required=False)\n",
    "#     parser.add_argument('--seed', default=0, type=int,\n",
    "#                         help='Random seed for reproducable results.', required=False)\n",
    "#     parser.add_argument('--cudnn-ctc', action='store_true',\n",
    "#                         help='Switches CTC backend from torch to cudnn')\n",
    "#     parser.add_argument('--njobs', default=32, type=int,\n",
    "#                         help='Number of threads for dataloader/decoding.', required=False)\n",
    "#     parser.add_argument('--cpu', action='store_true', help='Disable GPU training.')\n",
    "#     parser.add_argument('--no-pin', action='store_true',\n",
    "#                         help='Disable pin-memory for dataloader')\n",
    "#     parser.add_argument('--test', action='store_true', help='Test the model.')\n",
    "#     parser.add_argument('--no-msg', action='store_true', help='Hide all messages.')\n",
    "#     parser.add_argument('--lm', action='store_true',\n",
    "#                         help='Option for training RNNLM.')\n",
    "#     # Following features in development.\n",
    "#     parser.add_argument('--amp', action='store_true', help='Option to enable AMP.')\n",
    "#     parser.add_argument('--reserve-gpu', default=0, type=float,\n",
    "#                         help='Option to reserve GPU ram for training.')\n",
    "#     parser.add_argument('--jit', action='store_true',\n",
    "#                         help='Option for enabling jit in pytorch. (feature in development)')\n",
    "#     ###\n",
    "#     paras = parser.parse_args()\n",
    "    paras = Para()\n",
    "#     paras.config = './config/aishell_asr_example_lstm4atthead1-test.yaml'\n",
    "#     paras.name = None\n",
    "#     paras.logdir = 'log/'\n",
    "#     paras.ckpdir = 'ckpt/'\n",
    "#     paras.outdir = 'result/'\n",
    "#     paras.load = None\n",
    "#     paras.seed = 0\n",
    "#     paras.cudnn_ctc = False\n",
    "#     paras.cpu = False\n",
    "#     paras.no_pin = False\n",
    "#     paras.test = True\n",
    "#     paras.no_msg = False\n",
    "#     paras.lm = False\n",
    "#     paras.amp = False\n",
    "#     paras.reserve_gpu = 0\n",
    "#     paras.jit = False\n",
    "    setattr(paras, 'config', './config/cv11Lu_asr_lstm4atthead_allvocab-whisperclass.yaml')\n",
    "    setattr(paras, 'name', None)\n",
    "    setattr(paras, 'logdir', 'log/')\n",
    "    setattr(paras, 'ckpdir', 'LAS_ckpt/')\n",
    "    setattr(paras, 'outdir', 'result/')\n",
    "    setattr(paras, 'load', None)\n",
    "    setattr(paras, 'seed', 0)\n",
    "    setattr(paras, 'cudnn_ctc', False)\n",
    "    setattr(paras, 'njobs',0)\n",
    "    setattr(paras, 'cpu', False)\n",
    "    setattr(paras, 'no_pin', False)\n",
    "    setattr(paras, 'test', False)\n",
    "    setattr(paras, 'no_msg', False)\n",
    "    setattr(paras, 'lm', False)\n",
    "    setattr(paras, 'amp', False)\n",
    "    setattr(paras, 'reserve_gpu', 9)\n",
    "    setattr(paras, 'jit', False)\n",
    "    setattr(paras, 'gpu', not paras.cpu)\n",
    "    setattr(paras, 'pin_memory', not paras.no_pin)\n",
    "    setattr(paras, 'verbose', not paras.no_msg)\n",
    "    setattr(paras, 'finetune', False)\n",
    "    setattr(paras, 'binaryClass', True)\n",
    "#     force_cudnn_initialization()\n",
    "    print(torch.multiprocessing.get_start_method())\n",
    "#     torch.multiprocessing.set_start_method('spawn',force=True)\n",
    "    config = yaml.load(open(paras.config, 'r'), Loader=yaml.FullLoader)\n",
    "\n",
    "    np.random.seed(paras.seed)\n",
    "    torch.manual_seed(paras.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(paras.seed)\n",
    "\n",
    "    # Hack to preserve GPU ram just incase OOM later on server\n",
    "    if paras.gpu and paras.reserve_gpu > 0:\n",
    "        buff = torch.randn(int(paras.reserve_gpu * 1e9 // 4)).cuda()\n",
    "        del buff\n",
    "\n",
    "    if paras.lm:\n",
    "        # Train RNNLM\n",
    "        from train_lm import Solver\n",
    "\n",
    "        mode = 'train'\n",
    "    else:\n",
    "        if paras.test:\n",
    "            # Test ASR\n",
    "            assert paras.load is None, 'Load option is mutually exclusive to --test'\n",
    "            from test_asr import Solver\n",
    "\n",
    "            mode = 'test'\n",
    "        elif paras.finetune:\n",
    "            assert paras.load is not None\n",
    "            from finetune_asr import Solver\n",
    "            mode = 'train'\n",
    "        elif paras.binaryClass:\n",
    "            # from train_bertclass import Solver\n",
    "            from Whisper_Biclass_bert_train import Solver\n",
    "            mode = 'train'\n",
    "        else:\n",
    "            # Train ASR\n",
    "            from train_asr import Solver\n",
    "\n",
    "            mode = 'train'\n",
    "\n",
    "    print(\"\\nUsing {} mode\\n\".format(mode))\n",
    "\n",
    "    for idx in range(0,5):\n",
    "        paras.config = f'./config/cv11Lu_asr_lstm4atthead_allvocab-biclass2-5fold-{idx+1}.yaml'#latest7k_4lstm.pth\n",
    "        # setattr(paras, 'load', f'./ckpt/cv11Lu_asr_lstm4atthead_allvocab-biclass2-5fold-{idx+1}_sd0/bert8k.pth')\n",
    "        #要用vag01要改load_ckpt\n",
    "        config = yaml.load(open(paras.config, 'r'), Loader=yaml.FullLoader)\n",
    "        solver = Solver(config, paras, mode)\n",
    "\n",
    "\n",
    "        solver.load_data()\n",
    "        \n",
    "#         solver.print_model()\n",
    "        solver.set_model()\n",
    "        solver.exec()\n",
    "        del solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "traditional-cabinet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spawn\n",
      "\n",
      "Using train mode\n",
      "\n",
      "[INFO] Exp. name : cv11Lu_asr_lstm4atthead_allvocab-biclass2-5fold-1_sd0                                   \n",
      "[INFO] Loading data... large dataset may took a while.                                                     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-chinese were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data for training/validation, store tokenizer and input/output shape\n",
      "Prepare dataloader for training/validation\n",
      "Interface for creating all kinds of dataset\n",
      "import VAGDataset as Dataset\n",
      "[VAGDataset] path: data_process, split: ['CTT5-1-16']\n",
      "Mozillacv11Dataset CTT5-1-16 found wav data: 17\n",
      "text len: 17\n",
      "remove None, then wav data: 17, text len: 17\n",
      "[VAGDataset] path: data_process, split: ['CTT5-5-16', 'CTT5-2-16', 'CTT5-3-16', 'CTT5-4-16']\n",
      "Mozillacv11Dataset CTT5-5-16 found wav data: 17\n",
      "Mozillacv11Dataset CTT5-2-16 found wav data: 18\n",
      "Mozillacv11Dataset CTT5-3-16 found wav data: 17\n",
      "Mozillacv11Dataset CTT5-4-16 found wav data: 17\n",
      "text len: 69\n",
      "remove None, then wav data: 69, text len: 69\n",
      "Setup ASR model and optimizer \n",
      "# Losses\n",
      "# Note: zero_infinity=False is unstable?\n",
      "# Optimizer\n",
      "[INFO] Optim.spec.| Algo. = Adadelta\t| Lr = 1.0\t (Scheduler = fixed)| Scheduled sampling = False           \n",
      "# Enable AMP if needed\n",
      "fc.weight\n",
      "fc.bias\n",
      "Training End-to-end ASR system\n",
      "[INFO] Total training steps 1.0K.                                                                          \n",
      "self.tr_set: 69\n",
      "total_loss tensor(43.8096, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(43.3188, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(44.3357, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(42.9883, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(43.2393, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(42.3265, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(41.4746, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(40.9574, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(40.0442, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(38.6398, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(39.1160, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(38.7219, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(37.4811, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(36.7719, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(36.5311, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(35.7864, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(35.0999, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(34.9828, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(34.1582, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(33.6092, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(33.7798, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(32.7628, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(32.4133, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(31.8825, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(31.7272, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(31.4081, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(30.7672, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(30.6413, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(30.1786, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(29.7638, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(29.1040, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(29.5309, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(28.9623, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(28.8184, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(28.2079, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(27.9905, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(27.3071, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(27.6655, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(27.3084, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(27.0615, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(26.8704, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(26.1745, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(26.4072, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(26.1798, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(25.4655, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(25.6652, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(25.3903, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(25.2998, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(25.1490, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(24.9236, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(24.6400, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(24.5394, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(24.0609, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(24.0485, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(23.8376, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(23.8746, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(23.6849, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(23.4759, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(23.3693, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(22.9992, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(22.9763, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(22.7562, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(22.8557, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(22.4358, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(22.4039, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(22.3318, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(22.0720, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(22.0737, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(21.8689, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(21.6881, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(21.5606, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(21.5237, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(21.4403, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(21.3391, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.9413, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(21.1336, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(21.0778, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.7564, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.7095, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.6457, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.4482, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.4070, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.2829, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.2003, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.0247, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.0641, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.9893, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.6676, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.7750, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.7320, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.5366, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.5730, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.3938, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.4105, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.1849, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.2082, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.9873, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.8630, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.8628, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.9421, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.6275, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.5558, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.4597, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.5013, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.6491, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.2515, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.0634, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.2750, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.2010, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.1025, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.9601, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.7478, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.9351, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.7276, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.7103, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.6338, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.5956, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.4859, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.4636, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.2924, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.2943, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.4485, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.3130, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.1438, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.0297, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.0155, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.0148, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.8901, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.8255, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.7033, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.6622, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.6974, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.6199, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.5436, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.4940, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.5045, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.3921, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.2991, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.3831, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.1992, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.2123, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.2143, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.9975, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.0434, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.8394, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.9532, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.9154, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.8584, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.7355, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.6676, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.6830, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.6085, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.6871, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.5845, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.5593, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.3840, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.4707, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.2607, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.3449, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.2407, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.2335, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.1432, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.2220, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.1942, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.9453, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.0009, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.0310, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.8734, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.8823, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.9105, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.8147, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.7143, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.6971, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.6886, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.6803, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.6498, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.6167, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.4644, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.5354, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.3703, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.5081, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.3232, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.3619, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.3636, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.3312, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.3297, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.1333, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.1415, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.0662, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.0767, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.1764, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.0227, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.0134, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.9300, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.8893, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.9211, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.8553, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.8941, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.7166, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.7229, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.7434, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.7817, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.7642, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.5144, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.5394, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.6043, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.5747, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.5381, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.5073, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.4198, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.3908, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.3628, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.3630, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.2746, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.1978, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.2810, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.1915, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "15001\n",
      "[INFO] Exp. name : cv11Lu_asr_lstm4atthead_allvocab-biclass2-5fold-2_sd0                                   \n",
      "[INFO] Loading data... large dataset may took a while.                                                     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-chinese were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data for training/validation, store tokenizer and input/output shape\n",
      "Prepare dataloader for training/validation\n",
      "Interface for creating all kinds of dataset\n",
      "import VAGDataset as Dataset\n",
      "[VAGDataset] path: data_process, split: ['CTT5-2-16']\n",
      "Mozillacv11Dataset CTT5-2-16 found wav data: 18\n",
      "text len: 18\n",
      "remove None, then wav data: 18, text len: 18\n",
      "[VAGDataset] path: data_process, split: ['CTT5-1-16', 'CTT5-5-16', 'CTT5-3-16', 'CTT5-4-16']\n",
      "Mozillacv11Dataset CTT5-1-16 found wav data: 17\n",
      "Mozillacv11Dataset CTT5-5-16 found wav data: 17\n",
      "Mozillacv11Dataset CTT5-3-16 found wav data: 17\n",
      "Mozillacv11Dataset CTT5-4-16 found wav data: 17\n",
      "text len: 68\n",
      "remove None, then wav data: 68, text len: 68\n",
      "Setup ASR model and optimizer \n",
      "# Losses\n",
      "# Note: zero_infinity=False is unstable?\n",
      "# Optimizer\n",
      "[INFO] Optim.spec.| Algo. = Adadelta\t| Lr = 1.0\t (Scheduler = fixed)| Scheduled sampling = False           \n",
      "# Enable AMP if needed\n",
      "fc.weight\n",
      "fc.bias\n",
      "Training End-to-end ASR system\n",
      "[INFO] Total training steps 1.0K.                                                                          \n",
      "self.tr_set: 68\n",
      "total_loss tensor(53.7253, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(44.8058, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(44.8304, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(44.4591, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(44.1909, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(44.5819, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(43.6264, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(43.0956, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(42.8942, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(41.6686, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(41.1733, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(40.8172, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(39.1226, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(38.7521, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(38.6833, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(37.7546, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(36.7177, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(36.2980, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(36.4528, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(35.6055, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(34.7727, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(34.3045, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(33.7667, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(33.4725, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(33.0225, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(32.9347, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(31.9321, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(31.2977, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(30.8058, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(30.3393, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(30.3273, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(29.1862, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(29.5153, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(28.7178, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(28.7353, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(27.9395, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(27.7664, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(27.2634, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(27.6359, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(26.7646, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(26.2553, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(26.3560, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(25.7610, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(25.7002, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(25.1727, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(25.3266, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(24.6987, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(24.3404, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(24.6516, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(23.7803, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(23.9591, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(23.6188, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(23.6217, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(22.9736, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(23.1167, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(22.9161, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(22.6741, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(22.4681, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(21.9931, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(22.2711, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(21.7085, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(21.6383, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(21.2718, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(21.5330, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(21.1400, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(21.1815, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(20.6686, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(20.5611, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(20.3449, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(20.8412, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(20.4676, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(20.0415, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(20.1252, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(19.6531, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(19.6229, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(19.9121, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(19.4135, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(19.4949, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(19.4409, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(19.0972, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(18.9874, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(19.0441, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(18.7633, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(18.6141, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(18.7757, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(18.3834, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(18.1928, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(18.4385, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(18.1614, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(18.1851, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(18.0486, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(17.9697, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(17.8260, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(18.0103, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(17.6561, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(17.5749, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(17.2982, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(17.4550, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(17.4611, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(17.3584, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(17.0691, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(16.9830, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(16.9720, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(16.8659, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(16.9690, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(16.9015, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(16.7375, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(16.6686, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(16.6017, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(16.5792, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(16.5660, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(16.4570, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(16.4052, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(16.2527, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(15.9611, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(16.2558, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(16.0218, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(16.0845, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(16.1453, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(15.8733, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(15.7368, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(15.8480, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(15.9536, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(15.7888, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(15.6808, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(15.6847, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(15.4647, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(15.4448, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(15.5289, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(15.4175, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(15.2122, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(15.1429, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(15.3138, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(15.2177, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(15.1790, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(15.1927, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(15.0651, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(14.9973, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(15.0639, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(15.0136, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(14.7743, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(14.7109, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(14.7406, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(14.7482, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(14.7892, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(14.6940, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(14.7083, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(14.5057, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(14.5058, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(14.5165, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(14.5271, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(14.4316, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(14.3992, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(14.3049, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(14.2708, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(14.3931, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(14.3508, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(14.1206, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(14.2051, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(14.2943, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(14.1017, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(14.0716, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(14.0125, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(13.9324, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(13.9144, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(13.9282, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(13.8065, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(13.9165, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(13.7682, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(13.5745, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(13.6429, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(13.7581, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(13.7782, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(13.7999, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(13.7068, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(13.5063, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(13.5826, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(13.4331, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(13.5166, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(13.3896, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(13.4924, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(13.3376, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(13.4440, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(13.3153, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(13.3267, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(13.2324, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(13.2494, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(13.1559, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(13.4437, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(13.1657, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(13.0264, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(13.1935, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(12.9897, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(13.0676, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(13.1717, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(12.8861, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(12.8373, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(13.0418, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(12.9442, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(12.9438, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(12.8723, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(12.8863, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(12.8843, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(12.7747, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(12.6963, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(12.6774, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(12.7187, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(12.6549, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(12.5392, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(12.6299, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(12.5951, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(12.5674, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(12.7917, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(12.5852, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(12.5410, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(12.4617, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(12.5216, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(12.5118, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(12.5266, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "total_loss tensor(12.2606, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 68\n",
      "15001\n",
      "[INFO] Exp. name : cv11Lu_asr_lstm4atthead_allvocab-biclass2-5fold-3_sd0                                   \n",
      "[INFO] Loading data... large dataset may took a while.                                                     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-chinese were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data for training/validation, store tokenizer and input/output shape\n",
      "Prepare dataloader for training/validation\n",
      "Interface for creating all kinds of dataset\n",
      "import VAGDataset as Dataset\n",
      "[VAGDataset] path: data_process, split: ['CTT5-3-16']\n",
      "Mozillacv11Dataset CTT5-3-16 found wav data: 17\n",
      "text len: 17\n",
      "remove None, then wav data: 17, text len: 17\n",
      "[VAGDataset] path: data_process, split: ['CTT5-1-16', 'CTT5-2-16', 'CTT5-5-16', 'CTT5-4-16']\n",
      "Mozillacv11Dataset CTT5-1-16 found wav data: 17\n",
      "Mozillacv11Dataset CTT5-2-16 found wav data: 18\n",
      "Mozillacv11Dataset CTT5-5-16 found wav data: 17\n",
      "Mozillacv11Dataset CTT5-4-16 found wav data: 17\n",
      "text len: 69\n",
      "remove None, then wav data: 69, text len: 69\n",
      "Setup ASR model and optimizer \n",
      "# Losses\n",
      "# Note: zero_infinity=False is unstable?\n",
      "# Optimizer\n",
      "[INFO] Optim.spec.| Algo. = Adadelta\t| Lr = 1.0\t (Scheduler = fixed)| Scheduled sampling = False           \n",
      "# Enable AMP if needed\n",
      "fc.weight\n",
      "fc.bias\n",
      "Training End-to-end ASR system\n",
      "[INFO] Total training steps 1.0K.                                                                          \n",
      "self.tr_set: 69\n",
      "total_loss tensor(44.3645, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(44.1669, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(43.5377, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(43.2964, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(43.2034, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(42.4850, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(42.0418, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(41.2157, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(40.8263, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(39.4676, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(38.8914, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(38.4867, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(38.1493, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(37.2926, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(36.6601, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(35.4523, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(35.8084, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(35.2391, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(34.1181, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(34.3088, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(32.9654, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(32.6988, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(32.0876, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(32.1199, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(31.4298, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(31.1646, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(30.9051, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(30.2731, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(29.9567, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(29.5463, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(29.2686, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(28.5008, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(28.1899, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(28.2926, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(28.0965, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(27.7480, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(27.3563, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(26.8650, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(26.5273, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(26.6722, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(26.3871, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(25.7449, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(25.8034, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(25.3435, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(25.2216, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(25.1052, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(24.7758, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(24.2785, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(24.5347, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(24.3138, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(24.0329, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(23.7197, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(23.4779, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(23.3350, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(23.0838, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(23.2070, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(22.8914, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(22.7018, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(22.3700, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(22.4200, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(22.3420, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(21.9255, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(22.1110, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(21.6982, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(21.7131, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(21.2378, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(21.3583, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(21.4166, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(21.3518, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.9932, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.8206, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.7313, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.3151, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.6431, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.4368, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.3077, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.1677, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.9954, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.9690, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.8834, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.5988, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.6259, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.4552, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.4553, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.1399, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.3807, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.2214, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.0763, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.8999, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.8511, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.7023, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.7314, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.3908, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.6264, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.6404, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.1699, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.3998, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.2166, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.0619, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.1883, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.9280, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.9750, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.8298, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.8063, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.7644, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.6776, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.5536, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.4456, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.2315, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.3755, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.3217, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.2970, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.2146, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.1492, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.0450, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.9883, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.9450, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.9639, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.8588, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.8303, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.7302, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.6313, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.6084, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.5830, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.3789, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.3406, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.4780, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.2359, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.2929, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.2596, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.2421, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.1683, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.9004, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.1419, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.9506, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.9560, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.9408, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.8099, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.6951, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.7917, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.7442, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.5421, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.5851, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.5527, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.5646, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.4987, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.3095, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.3019, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.3287, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.2927, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.2975, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.2515, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.1777, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.0907, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.0545, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.8970, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.0601, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.9774, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.8676, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.9580, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.8979, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.7786, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.7181, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.7715, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.6295, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.6336, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.6128, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.6616, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.5080, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.4612, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.4489, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.3645, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.3904, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.4012, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.2704, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.2231, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.3724, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.2750, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.1449, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.1222, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.1516, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.9900, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.1177, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.0196, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.9185, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(14.0609, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.8565, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.8409, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.9710, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.7512, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.7937, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.6929, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.8049, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.7997, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.6039, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.6478, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.5748, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.5780, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.4858, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.5026, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.3989, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.4389, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.3583, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.4331, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.4099, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.3992, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.2711, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.3245, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.3216, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.2166, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.1663, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.0577, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.1961, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.0707, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.0771, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.1282, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(13.0815, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "15001\n",
      "[INFO] Exp. name : cv11Lu_asr_lstm4atthead_allvocab-biclass2-5fold-4_sd0                                   \n",
      "[INFO] Loading data... large dataset may took a while.                                                     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-chinese were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data for training/validation, store tokenizer and input/output shape\n",
      "Prepare dataloader for training/validation\n",
      "Interface for creating all kinds of dataset\n",
      "import VAGDataset as Dataset\n",
      "[VAGDataset] path: data_process, split: ['CTT5-4-16']\n",
      "Mozillacv11Dataset CTT5-4-16 found wav data: 17\n",
      "text len: 17\n",
      "remove None, then wav data: 17, text len: 17\n",
      "[VAGDataset] path: data_process, split: ['CTT5-1-16', 'CTT5-2-16', 'CTT5-3-16', 'CTT5-5-16']\n",
      "Mozillacv11Dataset CTT5-1-16 found wav data: 17\n",
      "Mozillacv11Dataset CTT5-2-16 found wav data: 18\n",
      "Mozillacv11Dataset CTT5-3-16 found wav data: 17\n",
      "Mozillacv11Dataset CTT5-5-16 found wav data: 17\n",
      "text len: 69\n",
      "remove None, then wav data: 69, text len: 69\n",
      "Setup ASR model and optimizer \n",
      "# Losses\n",
      "# Note: zero_infinity=False is unstable?\n",
      "# Optimizer\n",
      "[INFO] Optim.spec.| Algo. = Adadelta\t| Lr = 1.0\t (Scheduler = fixed)| Scheduled sampling = False           \n",
      "# Enable AMP if needed\n",
      "fc.weight\n",
      "fc.bias\n",
      "Training End-to-end ASR system\n",
      "[INFO] Total training steps 1.0K.                                                                          \n",
      "self.tr_set: 69\n",
      "total_loss tensor(58.9261, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(45.4746, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(43.3028, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(42.8384, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(44.1173, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(42.7621, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(42.0788, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(42.6611, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(40.7535, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(40.3517, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(40.5667, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(40.4780, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(40.0622, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(38.5556, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(38.1280, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(37.9107, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(37.9575, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(37.2030, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(36.7232, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(37.0177, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(36.2318, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(35.5068, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(35.2498, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(34.5987, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(34.7513, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(34.1400, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(33.7254, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(33.5539, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(32.7608, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(32.8875, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(32.2516, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(32.2000, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(31.8130, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(31.3664, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(31.0768, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(31.2405, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(30.4415, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(30.4787, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(30.1828, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(29.9242, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(29.7434, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(29.3186, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(28.8453, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(28.8015, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(28.8780, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(28.5108, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(28.3863, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(28.0020, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(27.7736, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(27.5725, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(27.2931, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(27.3229, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(27.0410, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(26.6895, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(26.7308, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(26.3489, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(26.0487, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(26.4308, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(25.9917, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(25.7877, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(25.7211, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(25.4551, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(25.1061, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(25.2608, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(24.9137, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(24.8118, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(24.7405, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(24.5946, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(24.5834, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(24.1679, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(24.0966, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(23.9744, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(24.0034, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(23.8626, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(23.5875, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(23.5629, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(23.4427, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(23.1154, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(23.0369, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(23.0990, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(22.9202, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(22.8700, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(22.4743, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(22.6376, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(22.5607, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(22.2806, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(22.3045, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(22.2349, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(21.8026, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(22.0237, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(21.9719, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(21.7892, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(21.6694, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(21.6281, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(21.4236, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(21.3795, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(21.1916, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(21.1026, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(21.1636, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.8398, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(21.0517, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.8759, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.6945, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.8461, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.6123, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.5879, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.3240, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.5575, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.2710, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.2182, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.1745, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.9877, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.6443, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.1058, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.9318, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.7113, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.8150, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.4685, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.6888, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.5522, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.5020, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.4102, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.2706, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.3222, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.2072, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.1320, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.9983, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.0224, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.8987, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.8489, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.7887, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.7566, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.7740, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.6359, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.5472, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.5431, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.4281, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.4687, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.3757, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.2421, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.3279, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.1909, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.1399, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.0867, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.0420, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.9757, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.8344, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.9177, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.8480, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.7508, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.7482, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.7371, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.4936, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.6543, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.5607, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.4884, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.3586, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.4630, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.4164, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.3302, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.2712, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.1824, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.1287, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.1332, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.1314, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.0607, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.8153, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.0279, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.9443, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.8336, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.7758, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.7963, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.6488, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.7428, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.5448, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.6622, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.5364, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.5729, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.5268, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.4537, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.4184, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.2978, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.3508, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.3358, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.2197, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.2454, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.0654, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.0923, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.1087, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.1136, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.0648, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.9372, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.9429, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.8585, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.8839, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.8262, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.8616, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.7804, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.7362, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.6300, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.6554, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.6699, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.5993, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.5233, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.4649, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.4677, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.4708, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.4643, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.3178, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.4065, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.3282, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.2626, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.2810, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.1714, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.0652, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.2415, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(15.1645, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "15001\n",
      "[INFO] Exp. name : cv11Lu_asr_lstm4atthead_allvocab-biclass2-5fold-5_sd0                                   \n",
      "[INFO] Loading data... large dataset may took a while.                                                     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-chinese were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data for training/validation, store tokenizer and input/output shape\n",
      "Prepare dataloader for training/validation\n",
      "Interface for creating all kinds of dataset\n",
      "import VAGDataset as Dataset\n",
      "[VAGDataset] path: data_process, split: ['CTT5-5-16']\n",
      "Mozillacv11Dataset CTT5-5-16 found wav data: 17\n",
      "text len: 17\n",
      "remove None, then wav data: 17, text len: 17\n",
      "[VAGDataset] path: data_process, split: ['CTT5-1-16', 'CTT5-2-16', 'CTT5-3-16', 'CTT5-4-16']\n",
      "Mozillacv11Dataset CTT5-1-16 found wav data: 17\n",
      "Mozillacv11Dataset CTT5-2-16 found wav data: 18\n",
      "Mozillacv11Dataset CTT5-3-16 found wav data: 17\n",
      "Mozillacv11Dataset CTT5-4-16 found wav data: 17\n",
      "text len: 69\n",
      "remove None, then wav data: 69, text len: 69\n",
      "Setup ASR model and optimizer \n",
      "# Losses\n",
      "# Note: zero_infinity=False is unstable?\n",
      "# Optimizer\n",
      "[INFO] Optim.spec.| Algo. = Adadelta\t| Lr = 1.0\t (Scheduler = fixed)| Scheduled sampling = False           \n",
      "# Enable AMP if needed\n",
      "fc.weight\n",
      "fc.bias\n",
      "Training End-to-end ASR system\n",
      "[INFO] Total training steps 1.0K.                                                                          \n",
      "self.tr_set: 69\n",
      "total_loss tensor(45.6131, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(42.9585, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(42.5761, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(42.0104, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(42.5214, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(41.9874, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(40.5817, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(40.5885, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(40.8354, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(39.7707, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(39.1523, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(38.7724, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(37.9506, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(38.1399, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(37.0423, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(37.0876, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(36.4829, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(36.2655, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(35.4021, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(34.7775, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(34.8656, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(34.8908, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(33.9024, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(33.7526, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(33.1191, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(33.0592, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(32.8195, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(32.2960, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(31.8976, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(31.8671, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(31.4537, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(31.1287, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(30.9342, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(30.7411, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(30.1322, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(30.3591, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(29.4742, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(29.8898, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(29.3926, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(29.0024, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(29.1253, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(28.3155, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(28.7228, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(27.7826, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(28.3053, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(27.9822, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(27.5912, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(27.5934, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(27.4310, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(27.0430, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(26.6617, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(26.9055, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(26.7271, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(26.2614, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(26.3191, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(26.0241, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(25.7237, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(25.8551, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(25.5854, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(25.5017, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(25.2688, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(25.0919, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(25.2202, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(24.7935, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(24.8312, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(24.5340, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(24.4522, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(24.3946, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(24.2241, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(24.2000, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(23.8911, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(24.0370, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(23.7816, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(23.5537, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(23.3815, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(23.3762, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(23.2014, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(23.1322, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(23.0792, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(23.0648, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(22.8494, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(22.7479, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(22.4294, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(22.5074, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(22.5488, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(22.4368, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(22.1645, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(22.0576, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(22.0907, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(22.0247, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(21.9835, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(21.8503, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(21.7185, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(21.7062, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(21.4725, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(21.4926, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(21.3635, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(21.3453, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(21.2481, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(21.1044, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(21.1037, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.9782, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.8635, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.8295, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.8562, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.7306, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.6247, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.7123, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.5695, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.3955, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.3477, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.2959, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.3124, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.0871, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(20.1948, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.8863, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.9917, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.8436, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.9359, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.8009, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.7167, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.7198, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.6122, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.5588, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.5526, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.4289, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.4064, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.3351, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.3627, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.2017, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.1956, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.1649, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.1554, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(19.0690, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.9624, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.9968, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.9069, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.8457, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.7501, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.7886, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.7443, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.5977, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.5371, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.5795, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.4013, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.6209, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.4806, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.3815, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.2414, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.3273, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.3423, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.1714, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.1862, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.1990, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.1326, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.9669, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(18.0865, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.9079, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.9220, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.8934, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.7756, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.8602, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.7877, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.7529, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.6813, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.6500, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.5475, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.6042, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.5405, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.4309, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.5173, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.4944, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.4520, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.3965, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.3123, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.2161, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.2395, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.1729, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.2498, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.1473, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.0615, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.1732, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.0398, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.9925, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.9300, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.8480, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(17.0228, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.8194, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.8736, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.7457, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.8561, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.7410, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.8631, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.6731, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.5472, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.7257, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.5466, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.5893, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.5461, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.5128, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.5083, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.3950, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.3881, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.4892, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.3442, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.3393, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.3229, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.3642, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.2720, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.2077, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.1527, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.1746, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.1645, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.0919, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.1117, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.0982, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "total_loss tensor(16.1360, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 69\n",
      "15001\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d2fe1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode about 30 sec\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import soundfile as sf\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = BertTokenizer.from_pretrained('shibing624/text2vec-base-chinese')\n",
    "model = BertModel.from_pretrained('shibing624/text2vec-base-chinese')\n",
    "sentences = ['如何更换花呗绑定银行卡', '花呗更改绑定银行卡']\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "# Perform pooling. In this case, mean pooling.\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "print(\"Sentence embeddings:\")\n",
    "print(sentence_embeddings)\n",
    "# path = \"data_process/CTTsegment_remove\"\n",
    "# transcriptionlist = pd.DataFrame()\n",
    "# for f in os.listdir(path):\n",
    "#     filepath = path + '/' + f\n",
    "#     if os.path.isfile(filepath) and f.find(\"wav\") != -1:\n",
    "#         print(\"filepath: \", filepath)\n",
    "#         wav, sr = sf.read(filepath)\n",
    "#         temp = \"data_process/CTTsegment_remove/temp.wav\"\n",
    "#         sf.write(temp, wav, 16000)\n",
    "#         wav, sr = sf.read(temp)\n",
    "#         print(wav, sr)\n",
    "#         # import librosa    \n",
    "#         # wav, sr = librosa.load(filepath, sr=16000) # Downsample 44.1kHz to 8kHz\n",
    "#         # print(sr)\n",
    "#         # outofCUDA\n",
    "#         # input_features = processor(wav, sampling_rate=sr, return_tensors=\"pt\").input_features.to(device)\n",
    "        \n",
    "#         input_values = processor(wav, sampling_rate=sr, return_tensors=\"pt\", padding=\"longest\").input_values.to(device)  # Batch size 1\n",
    "#         print(input_values.shape)\n",
    "#         # retrieve logits\n",
    "#         logits = model(input_values).last_hidden_state\n",
    "        \n",
    "#         # # take argmax and decode\n",
    "#         # predicted_ids = torch.argmax(logits, dim=-1)\n",
    "#         # transcription = processor.batch_decode(predicted_ids)\n",
    "\n",
    "#         # print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da76b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.cuda.device_count()\n",
    "import torch\n",
    "torch.cuda.is_available()\n",
    "# torch.cuda.device_count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-gregory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-cement",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python eval.py --file ~/LAS_Mandarin_PyTorch-master/result/mozillacv11_asr_stm4atthead-test_test_output.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2acf87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "\n",
    "# import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "# # os.environ['CUDA_VISIBLE_DEVICES']='0,1'\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING']='1'\n",
    "\n",
    "# torch.cuda.set_device(0)\n",
    "# print(torch.cuda.current_device())\n",
    "#! python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Author: kun\n",
    "# @Time: 2019-10-29 20:29\n",
    "\n",
    "import yaml\n",
    "# import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "class Para(object):\n",
    "    a=1\n",
    "\n",
    "def force_cudnn_initialization():\n",
    "    s = 32\n",
    "    dev = torch.device('cuda:0')\n",
    "    torch.nn.functional.conv2d(torch.zeros(s, s, s, s, device=dev), torch.zeros(s, s, s, s, device=dev))\n",
    "    \n",
    "#force_cudnn_initialization()\n",
    "def main():\n",
    "    # For reproducibility, comment these may speed up training\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Arguments\n",
    "#     parser = argparse.ArgumentParser(description='Training E2E asr.')\n",
    "#     parser.add_argument('--config', type=str, help='Path to experiment config.')\n",
    "#     parser.add_argument('--name', default=None, type=str, help='Name for logging.')\n",
    "#     parser.add_argument('--logdir', default='log/', type=str,\n",
    "#                         help='Logging path.', required=False)\n",
    "#     parser.add_argument('--ckpdir', default='ckpt/', type=str,\n",
    "#                         help='Checkpoint path.', required=False)\n",
    "#     parser.add_argument('--outdir', default='result/', type=str,\n",
    "#                         help='Decode output path.', required=False)\n",
    "#     parser.add_argument('--load', default=None, type=str,\n",
    "#                         help='Load pre-trained model (for training only)', required=False)\n",
    "#     parser.add_argument('--seed', default=0, type=int,\n",
    "#                         help='Random seed for reproducable results.', required=False)\n",
    "#     parser.add_argument('--cudnn-ctc', action='store_true',\n",
    "#                         help='Switches CTC backend from torch to cudnn')\n",
    "#     parser.add_argument('--njobs', default=32, type=int,\n",
    "#                         help='Number of threads for dataloader/decoding.', required=False)\n",
    "#     parser.add_argument('--cpu', action='store_true', help='Disable GPU training.')\n",
    "#     parser.add_argument('--no-pin', action='store_true',\n",
    "#                         help='Disable pin-memory for dataloader')\n",
    "#     parser.add_argument('--test', action='store_true', help='Test the model.')\n",
    "#     parser.add_argument('--no-msg', action='store_true', help='Hide all messages.')\n",
    "#     parser.add_argument('--lm', action='store_true',\n",
    "#                         help='Option for training RNNLM.')\n",
    "#     # Following features in development.\n",
    "#     parser.add_argument('--amp', action='store_true', help='Option to enable AMP.')\n",
    "#     parser.add_argument('--reserve-gpu', default=0, type=float,\n",
    "#                         help='Option to reserve GPU ram for training.')\n",
    "#     parser.add_argument('--jit', action='store_true',\n",
    "#                         help='Option for enabling jit in pytorch. (feature in development)')\n",
    "#     ###\n",
    "#     paras = parser.parse_args()\n",
    "    paras = Para()\n",
    "#     paras.config = './config/aishell_asr_example_lstm4atthead1-test.yaml'\n",
    "#     paras.name = None\n",
    "#     paras.logdir = 'log/'\n",
    "#     paras.ckpdir = 'ckpt/'\n",
    "#     paras.outdir = 'result/'\n",
    "#     paras.load = None\n",
    "#     paras.seed = 0\n",
    "#     paras.cudnn_ctc = False\n",
    "#     paras.cpu = False\n",
    "#     paras.no_pin = False\n",
    "#     paras.test = True\n",
    "#     paras.no_msg = False\n",
    "#     paras.lm = False\n",
    "#     paras.amp = False\n",
    "#     paras.reserve_gpu = 0\n",
    "#     paras.jit = False\n",
    "    setattr(paras, 'config', './config/cv11Lu_asr_lstm4atthead_allvocab-bertclass.yaml')\n",
    "    setattr(paras, 'name', None)\n",
    "    setattr(paras, 'logdir', 'log/')\n",
    "    setattr(paras, 'ckpdir', 'LAS_ckpt/')\n",
    "    setattr(paras, 'outdir', 'result/')\n",
    "    setattr(paras, 'load', None)\n",
    "    setattr(paras, 'seed', 0)\n",
    "    setattr(paras, 'cudnn_ctc', False)\n",
    "    setattr(paras, 'njobs',0)\n",
    "    setattr(paras, 'cpu', False)\n",
    "    setattr(paras, 'no_pin', False)\n",
    "    setattr(paras, 'test', False)\n",
    "    setattr(paras, 'no_msg', False)\n",
    "    setattr(paras, 'lm', False)\n",
    "    setattr(paras, 'amp', False)\n",
    "    setattr(paras, 'reserve_gpu', 9)\n",
    "    setattr(paras, 'jit', False)\n",
    "    setattr(paras, 'gpu', not paras.cpu)\n",
    "    setattr(paras, 'pin_memory', not paras.no_pin)\n",
    "    setattr(paras, 'verbose', not paras.no_msg)\n",
    "    setattr(paras, 'finetune', False)\n",
    "    setattr(paras, 'binaryClass', True)\n",
    "#     force_cudnn_initialization()\n",
    "    print(torch.multiprocessing.get_start_method())\n",
    "#     torch.multiprocessing.set_start_method('spawn',force=True)\n",
    "    config = yaml.load(open(paras.config, 'r'), Loader=yaml.FullLoader)\n",
    "\n",
    "    np.random.seed(paras.seed)\n",
    "    torch.manual_seed(paras.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(paras.seed)\n",
    "\n",
    "    # Hack to preserve GPU ram just incase OOM later on server\n",
    "    if paras.gpu and paras.reserve_gpu > 0:\n",
    "        buff = torch.randn(int(paras.reserve_gpu * 1e9 // 4)).cuda()\n",
    "        del buff\n",
    "\n",
    "    if paras.lm:\n",
    "        # Train RNNLM\n",
    "        from train_lm import Solver\n",
    "\n",
    "        mode = 'train'\n",
    "    else:\n",
    "        if paras.test:\n",
    "            # Test ASR\n",
    "            assert paras.load is None, 'Load option is mutually exclusive to --test'\n",
    "            from test_asr import Solver\n",
    "\n",
    "            mode = 'test'\n",
    "        elif paras.finetune:\n",
    "            assert paras.load is not None\n",
    "            from finetune_asr import Solver\n",
    "            mode = 'train'\n",
    "        elif paras.binaryClass:\n",
    "            # from train_bertclass import Solver\n",
    "            from Whisper_Biclass_bert_train import Solver\n",
    "            mode = 'train'\n",
    "        else:\n",
    "            # Train ASR\n",
    "            from train_asr import Solver\n",
    "\n",
    "            mode = 'train'\n",
    "\n",
    "    print(\"\\nUsing {} mode\\n\".format(mode))\n",
    "\n",
    "    for idx in range(0,5):\n",
    "        paras.config = f'./config/cv11Lu_asr_lstm4atthead_allvocab-biclass2-5fold-{idx+1}.yaml'#latest7k_4lstm.pth\n",
    "        setattr(paras, 'load', f'LAS_ckpt/cv11Lu_asr_lstm4atthead_allvocab-biclass2-5fold-{idx+1}_sd0/bert-v2-15k-r-final-2.pth')\n",
    "        #要用vag01要改load_ckpt\n",
    "        config = yaml.load(open(paras.config, 'r'), Loader=yaml.FullLoader)\n",
    "        solver = Solver(config, paras, mode)\n",
    "\n",
    "\n",
    "        solver.load_data()\n",
    "#         solver.print_model()\n",
    "        solver.set_model()\n",
    "        # solver.exec()\n",
    "        # solver.validate(idx)\n",
    "        solver.visualization()\n",
    "        del solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a571e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spawn\n",
      "\n",
      "Using train mode\n",
      "\n",
      "[INFO] Exp. name : cv11Lu_asr_lstm4atthead_allvocab-biclass2-5fold-1_sd0                                   \n",
      "[INFO] Loading data... large dataset may took a while.                                                     \n",
      "Load data for training/validation, store tokenizer and input/output shape\n",
      "Prepare dataloader for training/validation\n",
      "Interface for creating all kinds of dataset\n",
      "import VAGDataset as Dataset\n",
      "[VAGDataset] path: data_process, split: ['CTT5-1-16']\n",
      "Mozillacv11Dataset CTT5-1-16 found wav data: 17\n",
      "text len: 17\n",
      "remove None, then wav data: 17, text len: 17\n",
      "[VAGDataset] path: data_process, split: ['CTT5-5-16', 'CTT5-2-16', 'CTT5-3-16', 'CTT5-4-16']\n",
      "Mozillacv11Dataset CTT5-5-16 found wav data: 17\n",
      "Mozillacv11Dataset CTT5-2-16 found wav data: 18\n",
      "Mozillacv11Dataset CTT5-3-16 found wav data: 17\n",
      "Mozillacv11Dataset CTT5-4-16 found wav data: 17\n",
      "text len: 69\n",
      "remove None, then wav data: 69, text len: 69\n",
      "Setup ASR model and optimizer \n",
      "# Losses\n",
      "# Note: zero_infinity=False is unstable?\n",
      "# Optimizer\n",
      "[INFO] Optim.spec.| Algo. = Adadelta\t| Lr = 1.0\t (Scheduler = fixed)| Scheduled sampling = False           \n",
      "# Enable AMP if needed\n",
      "[INFO] Load ckpt from LAS_ckpt/cv11Lu_asr_lstm4atthead_allvocab-biclass2-5fold-1_sd0/bert-v2-15k-r-final-2.pth, restarting at step 15001 \n",
      "fc.weight\n",
      "fc.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output tensor([0.4526], device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\naomi\\anaconda3\\envs\\230917\\lib\\site-packages\\captum\\attr\\_utils\\visualization.py:338: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "c:\\Users\\naomi\\anaconda3\\envs\\230917\\lib\\site-packages\\torch\\utils\\tensorboard\\_utils.py:22: UserWarning: Glyph 8722 (\\N{MINUS SIGN}) missing from current font.\n",
      "  canvas.draw()\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BertModel' object has no attribute 'bert'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 163\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    160\u001b[0m solver\u001b[38;5;241m.\u001b[39mset_model()\n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# solver.exec()\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;66;03m# solver.validate(idx)\u001b[39;00m\n\u001b[1;32m--> 163\u001b[0m \u001b[43msolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualization\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m solver\n",
      "File \u001b[1;32md:\\Lulu\\Research\\0709\\LAS_Mandarin_PyTorch-master\\Whisper_Biclass_bert_train.py:541\u001b[0m, in \u001b[0;36mSolver.visualization\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    538\u001b[0m token_type_ids \u001b[38;5;241m=\u001b[39m typeid\n\u001b[0;32m    540\u001b[0m \u001b[38;5;66;03m# Get the embeddings\u001b[39;00m\n\u001b[1;32m--> 541\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mcombined_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    542\u001b[0m embeddings\u001b[38;5;241m.\u001b[39mrequires_grad_()\n\u001b[0;32m    544\u001b[0m \u001b[38;5;66;03m# Create a wrapper function for Saliency\u001b[39;00m\n",
      "File \u001b[1;32md:\\Lulu\\Research\\0709\\LAS_Mandarin_PyTorch-master\\Whisper_Biclass_bert_train.py:531\u001b[0m, in \u001b[0;36mSolver.visualization.<locals>.CombinedModel.get_embeddings\u001b[1;34m(self, input_ids, token_type_ids)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_embeddings\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, token_type_ids):\n\u001b[1;32m--> 531\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert_c\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[38;5;241m.\u001b[39membeddings(input_ids\u001b[38;5;241m=\u001b[39minput_ids, token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids)\n",
      "File \u001b[1;32mc:\\Users\\naomi\\anaconda3\\envs\\230917\\lib\\site-packages\\torch\\nn\\modules\\module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BertModel' object has no attribute 'bert'"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aedf8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-chinese were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\naomi\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "# os.environ['CUDA_VISIBLE_DEVICES']='0,1'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']='1'\n",
    "import torch\n",
    "# torch.cuda.set_device(0)\n",
    "print(torch.cuda.current_device())\n",
    "#! python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Author: kun\n",
    "# @Time: 2019-10-29 20:29\n",
    "\n",
    "import yaml\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import (\n",
    "    AutoModelForMaskedLM,\n",
    "    BertModel,\n",
    "    BertTokenizer,\n",
    "    AdamW,\n",
    "    BertForSequenceClassification\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "device = 'cuda:0'\n",
    "model_checkpoint = \"google-bert/bert-base-chinese\"\n",
    "bert = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "bert = bert.to(device)\n",
    "optimizer = AdamW(bert.parameters(), lr=5e-5)\n",
    "\n",
    "model_version = 'google-bert/bert-base-chinese'\n",
    "model = BertModel.from_pretrained(model_version, output_attentions=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_version)\n",
    "\n",
    "\n",
    "\n",
    "class Para(object):\n",
    "    a=1\n",
    "\n",
    "def list_5fold():\n",
    "    path_list = []\n",
    "    for idx in range(5):\n",
    "        path = \"data_process/CTT5-\" + str(idx+1) + \"-2\"\n",
    "        for f in os.listdir(path):\n",
    "            file = int(str(f).split(\"CTT\")[0].split('/')[-1])\n",
    "            path_list.append({\"file\": file, \"5fold\": idx+1})\n",
    "    return path_list\n",
    "\n",
    "def main():\n",
    "    # For reproducibility, comment these may speed up training\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    print(\"start\")\n",
    "    \n",
    "    path_list = pd.DataFrame(list_5fold())\n",
    "    path_list = path_list.sort_values(by='file', ascending=True)\n",
    "    # path_list.set_index(\"file\" , inplace=True)\n",
    "    path_list = path_list.reset_index()\n",
    "    # print(path_list)\n",
    "\n",
    "    txt = pd.read_csv('data_process/whisper-v2.csv')\n",
    "    # txt = txt.replace({'eval': {-1: 1}})\n",
    "    # print(txt)\n",
    "    txt[\"5fold\"] = path_list[\"5fold\"]\n",
    "    # txt[\"input\"] = [str(txt[\"AD\"][i]) + str(txt[\"transcription\"][i]) for i in range(len(txt))]\n",
    "\n",
    "    feature = txt.drop(columns = ['file', 'AD', '5fold']).astype(str).to_numpy()\n",
    "    # feature = txt[\"eval\"].to_numpy()\n",
    "    # feature = np.reshape(feature, (-1, 1))\n",
    "\n",
    "    # print(feature)\n",
    "\n",
    "    target = txt[\"AD\"].to_numpy()\n",
    "    target = np.reshape(target, (-1, 1))\n",
    "    # print(txt, feature, target)\n",
    "    \n",
    "    alldf =pd.DataFrame()\n",
    "    for fold in range(0,5):\n",
    "        # 5 fold\n",
    "        train = []\n",
    "        valid = []\n",
    "        t_tra = []\n",
    "        t_val = []\n",
    "        name = []\n",
    "        # y_predicted_val = []\n",
    "        idx = 0\n",
    "\n",
    "        for idx in range(feature.shape[0]):\n",
    "            if(int(txt[\"5fold\"][idx]) != int(str(fold+1))):\n",
    "                train.append(feature[idx])\n",
    "                t_tra.append(target[idx])\n",
    "            else:\n",
    "                valid.append(feature[idx])\n",
    "                t_val.append(target[idx])\n",
    "                # if(feature[idx]>=5):\n",
    "                #     y_predicted_val.append(0)\n",
    "                # else:\n",
    "                #     y_predicted_val.append(1)                    \n",
    "                name.append(txt[\"file\"][idx])\n",
    "\n",
    "        train = np.array(train)\n",
    "        t_tra = np.array(t_tra)\n",
    "        valid = np.array(valid)\n",
    "        t_val = np.array(t_val)\n",
    "        name = np.array(name)\n",
    "        # name = np.reshape(name, (-1, 1)) \n",
    "        \n",
    "        tokenizer = BertTokenizer.from_pretrained('google-bert/bert-base-chinese')\n",
    "        model = BertForSequenceClassification.from_pretrained('google-bert/bert-base-chinese', num_labels=len(set(torch.tensor(t_tra))))\n",
    "        print(\"fold: \", fold)\n",
    "         \n",
    "        for i in range(len(train)):\n",
    "            print(train[i])\n",
    "            train_encodings = tokenizer(train[i][0], truncation=True, padding=True, max_length=128)\n",
    "            labels = torch.tensor(t_tra[i]).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = torch.tensor(train_encodings['input_ids']).to(device)\n",
    "            attention_mask = torch.tensor(train_encodings['attention_mask']).to(device)\n",
    "            # labels = item['labels'].to(device)\n",
    "            print(input_ids, attention_mask, labels)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        predictions, true_labels = [], []\n",
    "        for i in range(len(valid)):\n",
    "            test_encodings = tokenizer(train[i][0], truncation=True, padding=True, max_length=128)\n",
    "            labels = torch.tensor(t_val[i]).to(device)\n",
    "\n",
    "            input_ids = torch.tensor(test_encodings['input_ids']).to(device)\n",
    "            attention_mask = torch.tensor(test_encodings['attention_mask']).to(device)\n",
    "            # labels = item['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            predictions.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "       # 計算準確率\n",
    "        accuracy = accuracy_score(true_labels, predictions)\n",
    "        print(f'Accuracy: {accuracy}')\n",
    "\n",
    "        # 顯示分類報告\n",
    "        print(classification_report(true_labels, predictions))\n",
    "\n",
    "#         # ds = DecisionTreeClassifier()\n",
    "#         # knn = KNeighborsClassifier()\n",
    "#         # svc = SVC(kernel='poly', C=1, gamma=\"auto\", degree=3, probability=True)\n",
    "#         svc = SVC(kernel='linear', C=100, gamma=\"auto\", probability=True)\n",
    "#         # clf = BaggingClassifier(base_estimator=svc, n_estimators=10, random_state=0)\n",
    "#         clf = make_pipeline(StandardScaler(), svc)\n",
    "\n",
    "#         # Fit the model to the data\n",
    "#         clf.fit(train, t_tra.ravel())\n",
    "#         # print(train, t_tra)\n",
    "# #         print(reg.coef_)\n",
    "#         y_predicted = clf.predict(train)\n",
    "# #         loss = mean_squared_error(t_tra, y_predicted)\n",
    "#         y_predicted_val = clf.predict(valid)\n",
    "#         y_predicted_val_pro = clf.predict_proba(valid)\n",
    "#         y_predicted_val_pro = [row[1] for row in y_predicted_val_pro]\n",
    "\n",
    "        # feature_import = pd.Series(imp, index=names).nlargest(4).plot(kind='barh')\n",
    "        # print(feature_import)\n",
    "\n",
    "        # y_predicted_val = np.reshape(y_predicted_val, (-1, 1)) \n",
    "#         loss_val = mean_squared_error(t_val, y_predicted_val)\n",
    "#         print(\"loss \", loss, \"loss_val\", loss_val)\n",
    "\n",
    "        \n",
    "        # pipeline = Pipeline([\n",
    "        #     ('scaler', StandardScaler()), \n",
    "        #     ('svc', SVC(probability=True, random_state=1))\n",
    "        # ])\n",
    "\n",
    "        # param_grid = {\n",
    "        #     'svc__kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "        #     'svc__C': [0.1, 1, 10, 100],\n",
    "        #     'svc__gamma': ['scale', 'auto'], \n",
    "        #     'svc__degree': [2, 3, 4] \n",
    "        # }\n",
    "\n",
    "        # grid_search = GridSearchCV(pipeline, param_grid, cv=5, verbose=2, n_jobs=-1)\n",
    "        # grid_search.fit(train, t_tra.ravel())\n",
    "\n",
    "        # print(\"Best Parameters: \", grid_search.best_params_)\n",
    "        # best_clf = grid_search.best_estimator_\n",
    "        # y_predicted_val = best_clf.predict(valid)\n",
    "\n",
    "        # imp = svc.coef_\n",
    "        # imp = abs(imp[0])\n",
    "        # print(imp)   \n",
    "        # names = ['fluency', 'richness', 'correlation', 'grammar', 'complexity']\n",
    "        # imp,names = zip(*sorted(zip(imp,names)))\n",
    "        # plt.barh(range(len(names)), imp, align='center')\n",
    "        # plt.yticks(range(len(names)), names)\n",
    "        # plt.show()\n",
    "\n",
    "        # train = np.reshape(train, (-1,))\n",
    "        # t_tra = np.reshape(t_tra, (-1,))\n",
    "        # valid = np.reshape(valid, (-1,))\n",
    "        # t_val = np.reshape(t_val, (-1,))\n",
    "        # y_predicted_val_pro = np.reshape(y_predicted_val_pro, (-1,))\n",
    "\n",
    "        # print('train:', train)\n",
    "        # print(t_tra)\n",
    "        # print(\"valid\", valid)\n",
    "        # print(y_predicted_val)\n",
    "\n",
    "        # plt.scatter(train, t_tra, cmap=plt.cm.coolwarm)\n",
    "        # plt.xlabel('Sepal length')\n",
    "        # plt.ylabel('Sepal width')\n",
    "        # plt.xticks(())\n",
    "        # plt.yticks(())\n",
    "        # plt.show()\n",
    "        \n",
    "        # print(name, y_predicted_val, t_val)\n",
    "#         df = pd.DataFrame(\n",
    "#             {'name': name,\n",
    "#              'result': y_predicted_val,\n",
    "#              'truth': t_val,\n",
    "#              'hyps': y_predicted_val_pro\n",
    "#             })\n",
    "#         alldf = pd.concat([alldf, df])#\n",
    "# #         print(df)\n",
    "#         # df.to_csv(f'biclass_result/5folds_biclass_{fold+1}_pause_e.csv', index=False)\n",
    "#     alldf.to_csv(\"biclass_result/bert-v2-combine.csv\", index=False) \n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442370c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold:  0\n",
      "['我看到一個小男生站在椅子上想要拿餅乾盒然後他看起來快要跌倒然後有一個小女孩在下面想要接他從餅乾盒拿出來的東西櫃子是打開的然後有一位女性正在洗碗朝前面洗碗然後水龍頭是打開的水已經滿出來了 流到地上然後有窗戶窗簾半開窗戶是打開的外面可以看到房子外有一些草木然後琉璃台上面還有幾個碗盤看不出來洗過了沒有這應該是一個床']\n",
      "tensor([ 101, 2769, 4692, 1168,  671,  943, 2207, 4511, 4495, 4991, 1762, 3488,\n",
      "        2094,  677, 2682, 6206, 2897, 7619,  746, 4665, 4197, 2527,  800, 4692,\n",
      "        6629,  889, 2571, 6206, 6649,  948, 4197, 2527, 3300,  671,  943, 2207,\n",
      "        1957, 2111, 1762,  678, 7481, 2682, 6206, 2970,  800, 2537, 7619,  746,\n",
      "        4665, 2897, 1139,  889, 4638, 3346, 6205, 3602, 2094, 3221, 2802, 7274,\n",
      "        4638, 4197, 2527, 3300,  671,  855, 1957, 2595, 3633, 1762, 3819, 4813,\n",
      "        3308, 1184, 7481, 3819, 4813, 4197, 2527, 3717, 7983, 7531, 3221, 2802,\n",
      "        7274, 4638, 3717, 2347, 5195, 4021, 1139,  889,  749, 3837, 1168, 1765,\n",
      "         677, 4197, 2527, 3300, 4970, 2786, 4970, 5088, 1288, 7274, 4970, 2786,\n",
      "        3221, 2802, 7274, 4638, 1912, 7481, 1377,  809, 4692, 1168, 2791, 2094,\n",
      "        1912, 3300,  671,  763, 5770, 3312, 4197,  102], device='cuda:0') tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0') tensor([0], device='cuda:0')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 136\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# labels = item['labels'].to(device)\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28mprint\u001b[39m(input_ids, attention_mask, labels)\n\u001b[1;32m--> 136\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m    138\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\naomi\\anaconda3\\envs\\230917\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1693\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1685\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1686\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1687\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1688\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1689\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1690\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1691\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1693\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1694\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1695\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1696\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1699\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1700\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1701\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1702\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1703\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1705\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1707\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32mc:\\Users\\naomi\\anaconda3\\envs\\230917\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1061\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1059\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify either input_ids or inputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1061\u001b[0m batch_size, seq_length \u001b[38;5;241m=\u001b[39m input_shape\n\u001b[0;32m   1062\u001b[0m device \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;28;01mif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m inputs_embeds\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m   1064\u001b[0m \u001b[38;5;66;03m# past_key_values_length\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c288c273",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "llama_v2_hyps = llama_v2['hyps'].tolist()\n",
    "llama_v2_result = llama_v2['result'].tolist()\n",
    "llama_v2_truth = llama_v2['truth'].tolist()\n",
    "\n",
    "llama_v2_wrong = llama_v2.iloc[np.where(llama_v2['result'] != llama_v2['truth'])]\n",
    "print(llama_v2_wrong)\n",
    "\n",
    "target_names = ['non-patient', 'patient']\n",
    "clf_report = metrics.classification_report(llama_v2_truth, llama_v2_result, target_names=target_names)\n",
    "print(clf_report)\n",
    "\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(llama_v2_truth, llama_v2_result).ravel()\n",
    "print('sensitivuty: ',tp / (fn+tp))\n",
    "print('specificity: ',tn / (fp+tn))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(np.array(llama_v2_truth), np.array(llama_v2_hyps), pos_label=1)\n",
    "print('AUC: ',metrics.auc(fpr, tpr))\n",
    "fpr1, tpr1, thresholds1 = metrics.roc_curve(np.array(llama_v2_truth[0:19]), np.array(llama_v2_hyps[0:19]), pos_label=1)\n",
    "auc1 = metrics.auc(fpr1, tpr1)\n",
    "fpr2, tpr2, thresholds2 = metrics.roc_curve(np.array(llama_v2_truth[19:37]), np.array(llama_v2_hyps[19:37]), pos_label=1)\n",
    "auc2 = metrics.auc(fpr2, tpr2)\n",
    "fpr3, tpr3, thresholds3 = metrics.roc_curve(np.array(llama_v2_truth[37:54]), np.array(llama_v2_hyps[37:54]), pos_label=1)\n",
    "auc3 = metrics.auc(fpr3, tpr3)\n",
    "fpr4, tpr4, thresholds4 = metrics.roc_curve(np.array(llama_v2_truth[54:71]), np.array(llama_v2_hyps[54:71]), pos_label=1)\n",
    "auc4 = metrics.auc(fpr4, tpr4)\n",
    "fpr5, tpr5, thresholds5 = metrics.roc_curve(np.array(llama_v2_truth[71:88]), np.array(llama_v2_hyps[71:88]), pos_label=1)\n",
    "auc5 = metrics.auc(fpr5, tpr5)\n",
    "print('AUC: ', auc1, auc2, auc3, auc4, auc5)\n",
    "print(\"AUC_avg: \", (auc1+auc2+auc3+auc4+auc5)/5)\n",
    "\n",
    "y_test = [bool(int(x)) for x in llama_v2_truth]\n",
    "y_pred = [bool(int(x)) for x in llama_v2_result]\n",
    "metrics.ConfusionMatrixDisplay.from_predictions(y_test, y_pred, cmap=\"Blues\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
