{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "graduate-business",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "\n",
    "# import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "# # os.environ['CUDA_VISIBLE_DEVICES']='0,1'\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING']='1'\n",
    "\n",
    "# torch.cuda.set_device(0)\n",
    "# print(torch.cuda.current_device())\n",
    "#! python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Author: kun\n",
    "# @Time: 2019-10-29 20:29\n",
    "\n",
    "import yaml\n",
    "# import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "class Para(object):\n",
    "    a=1\n",
    "\n",
    "def force_cudnn_initialization():\n",
    "    s = 32\n",
    "    dev = torch.device('cuda:0')\n",
    "    torch.nn.functional.conv2d(torch.zeros(s, s, s, s, device=dev), torch.zeros(s, s, s, s, device=dev))\n",
    "    \n",
    "#force_cudnn_initialization()\n",
    "def main():\n",
    "    # For reproducibility, comment these may speed up training\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Arguments\n",
    "#     parser = argparse.ArgumentParser(description='Training E2E asr.')\n",
    "#     parser.add_argument('--config', type=str, help='Path to experiment config.')\n",
    "#     parser.add_argument('--name', default=None, type=str, help='Name for logging.')\n",
    "#     parser.add_argument('--logdir', default='log/', type=str,\n",
    "#                         help='Logging path.', required=False)\n",
    "#     parser.add_argument('--ckpdir', default='ckpt/', type=str,\n",
    "#                         help='Checkpoint path.', required=False)\n",
    "#     parser.add_argument('--outdir', default='result/', type=str,\n",
    "#                         help='Decode output path.', required=False)\n",
    "#     parser.add_argument('--load', default=None, type=str,\n",
    "#                         help='Load pre-trained model (for training only)', required=False)\n",
    "#     parser.add_argument('--seed', default=0, type=int,\n",
    "#                         help='Random seed for reproducable results.', required=False)\n",
    "#     parser.add_argument('--cudnn-ctc', action='store_true',\n",
    "#                         help='Switches CTC backend from torch to cudnn')\n",
    "#     parser.add_argument('--njobs', default=32, type=int,\n",
    "#                         help='Number of threads for dataloader/decoding.', required=False)\n",
    "#     parser.add_argument('--cpu', action='store_true', help='Disable GPU training.')\n",
    "#     parser.add_argument('--no-pin', action='store_true',\n",
    "#                         help='Disable pin-memory for dataloader')\n",
    "#     parser.add_argument('--test', action='store_true', help='Test the model.')\n",
    "#     parser.add_argument('--no-msg', action='store_true', help='Hide all messages.')\n",
    "#     parser.add_argument('--lm', action='store_true',\n",
    "#                         help='Option for training RNNLM.')\n",
    "#     # Following features in development.\n",
    "#     parser.add_argument('--amp', action='store_true', help='Option to enable AMP.')\n",
    "#     parser.add_argument('--reserve-gpu', default=0, type=float,\n",
    "#                         help='Option to reserve GPU ram for training.')\n",
    "#     parser.add_argument('--jit', action='store_true',\n",
    "#                         help='Option for enabling jit in pytorch. (feature in development)')\n",
    "#     ###\n",
    "#     paras = parser.parse_args()\n",
    "    paras = Para()\n",
    "#     paras.config = './config/aishell_asr_example_lstm4atthead1-test.yaml'\n",
    "#     paras.name = None\n",
    "#     paras.logdir = 'log/'\n",
    "#     paras.ckpdir = 'ckpt/'\n",
    "#     paras.outdir = 'result/'\n",
    "#     paras.load = None\n",
    "#     paras.seed = 0\n",
    "#     paras.cudnn_ctc = False\n",
    "#     paras.cpu = False\n",
    "#     paras.no_pin = False\n",
    "#     paras.test = True\n",
    "#     paras.no_msg = False\n",
    "#     paras.lm = False\n",
    "#     paras.amp = False\n",
    "#     paras.reserve_gpu = 0\n",
    "#     paras.jit = False\n",
    "    setattr(paras, 'config', './config/cv11Lu_asr_lstm4atthead_allvocab-whisperclass.yaml')\n",
    "    setattr(paras, 'name', None)\n",
    "    setattr(paras, 'logdir', 'log/')\n",
    "    setattr(paras, 'ckpdir', 'LAS_ckpt/')\n",
    "    setattr(paras, 'outdir', 'result/')\n",
    "    setattr(paras, 'load', None)\n",
    "    setattr(paras, 'seed', 0)\n",
    "    setattr(paras, 'cudnn_ctc', False)\n",
    "    setattr(paras, 'njobs',0)\n",
    "    setattr(paras, 'cpu', False)\n",
    "    setattr(paras, 'no_pin', False)\n",
    "    setattr(paras, 'test', False)\n",
    "    setattr(paras, 'no_msg', False)\n",
    "    setattr(paras, 'lm', False)\n",
    "    setattr(paras, 'amp', False)\n",
    "    setattr(paras, 'reserve_gpu', 9)\n",
    "    setattr(paras, 'jit', False)\n",
    "    setattr(paras, 'gpu', not paras.cpu)\n",
    "    setattr(paras, 'pin_memory', not paras.no_pin)\n",
    "    setattr(paras, 'verbose', not paras.no_msg)\n",
    "    setattr(paras, 'finetune', False)\n",
    "    setattr(paras, 'binaryClass', True)\n",
    "#     force_cudnn_initialization()\n",
    "    print(torch.multiprocessing.get_start_method())\n",
    "#     torch.multiprocessing.set_start_method('spawn',force=True)\n",
    "    config = yaml.load(open(paras.config, 'r'), Loader=yaml.FullLoader)\n",
    "\n",
    "    np.random.seed(paras.seed)\n",
    "    torch.manual_seed(paras.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(paras.seed)\n",
    "\n",
    "    # Hack to preserve GPU ram just incase OOM later on server\n",
    "    if paras.gpu and paras.reserve_gpu > 0:\n",
    "        buff = torch.randn(int(paras.reserve_gpu * 1e9 // 4)).cuda()\n",
    "        del buff\n",
    "\n",
    "    if paras.lm:\n",
    "        # Train RNNLM\n",
    "        from train_lm import Solver\n",
    "\n",
    "        mode = 'train'\n",
    "    else:\n",
    "        if paras.test:\n",
    "            # Test ASR\n",
    "            assert paras.load is None, 'Load option is mutually exclusive to --test'\n",
    "            from test_asr import Solver\n",
    "\n",
    "            mode = 'test'\n",
    "        elif paras.finetune:\n",
    "            assert paras.load is not None\n",
    "            from finetune_asr import Solver\n",
    "            mode = 'train'\n",
    "        elif paras.binaryClass:\n",
    "            # from train_whisperclass import Solver\n",
    "            from Whisper_Biclass_concat_train import Solver\n",
    "            mode = 'train'\n",
    "        else:\n",
    "            # Train ASR\n",
    "            from train_asr import Solver\n",
    "\n",
    "            mode = 'train'\n",
    "\n",
    "    print(\"\\nUsing {} mode\\n\".format(mode))\n",
    "\n",
    "    for idx in range(0,5):\n",
    "        paras.config = f'./config/cv11Lu_asr_lstm4atthead_allvocab-biclass2-5fold-{idx+1}.yaml'#latest7k_4lstm.pth\n",
    "        # setattr(paras, 'load', f'./ckpt/cv11Lu_asr_lstm4atthead_allvocab-biclass2-5fold-{idx+1}_sd0/whisper8k.pth')\n",
    "        #要用vag01要改load_ckpt\n",
    "        config = yaml.load(open(paras.config, 'r'), Loader=yaml.FullLoader)\n",
    "        solver = Solver(config, paras, mode)\n",
    "\n",
    "\n",
    "        solver.load_data()\n",
    "        # solver.load_data_2()\n",
    "        # solver.print_model()\n",
    "        solver.set_model()\n",
    "        solver.exec()\n",
    "        del solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "traditional-cabinet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spawn\n",
      "\n",
      "Using train mode\n",
      "\n",
      "[INFO] Exp. name : cv11Lu_asr_lstm4atthead_allvocab-biclass2-5fold-1_sd0                                   \n",
      "[INFO] Loading data... large dataset may took a while.                                                     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\naomi\\anaconda3\\envs\\230917\\lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of WhisperForAudioClassification were not initialized from the model checkpoint at sanchit-gandhi/whisper-large-v2-ft-ls-960h and are newly initialized: ['model.classifier.bias', 'model.classifier.weight', 'model.projector.bias', 'model.projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at google-bert/bert-base-chinese were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data for training/validation, store tokenizer and input/output shape\n",
      "Prepare dataloader for training/validation\n",
      "Interface for creating all kinds of dataset\n",
      "import VAGDataset as Dataset\n",
      "[VAGDataset] path: data_process, split: ['CTT5-1-15']\n",
      "Mozillacv11Dataset CTT5-1-15 found wav data: 51\n",
      "text len: 51\n",
      "remove None, then wav data: 51, text len: 51\n",
      "[VAGDataset] path: data_process, split: ['CTT5-5-15', 'CTT5-2-15', 'CTT5-3-15', 'CTT5-4-15']\n",
      "Mozillacv11Dataset CTT5-5-15 found wav data: 45\n",
      "Mozillacv11Dataset CTT5-2-15 found wav data: 41\n",
      "Mozillacv11Dataset CTT5-3-15 found wav data: 50\n",
      "Mozillacv11Dataset CTT5-4-15 found wav data: 45\n",
      "text len: 181\n",
      "remove None, then wav data: 181, text len: 181\n",
      "Setup ASR model and optimizer \n",
      "# Losses\n",
      "# Note: zero_infinity=False is unstable?\n",
      "# Optimizer\n",
      "[INFO] Optim.spec.| Algo. = Adadelta\t| Lr = 1.0\t (Scheduler = fixed)| Scheduled sampling = False           \n",
      "# Enable AMP if needed\n",
      "fc.weight\n",
      "fc.bias\n",
      "Training End-to-end ASR system\n",
      "[INFO] Total training steps 1.0K.                                                                          \n",
      "self.tr_set: 181\n",
      "total_loss tensor(122.5350, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(118.4214, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(118.9965, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(116.8698, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(115.4668, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(114.1256, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(112.7653, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(111.1488, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(109.9867, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(108.7451, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(107.5646, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(106.2769, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(105.3082, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(104.2756, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(103.1897, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(102.4405, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(101.3571, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(99.6334, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(100.1383, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(98.7836, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(97.8072, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(96.7669, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(96.5572, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(95.4952, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(94.4635, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(94.1965, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(93.0799, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(92.9642, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(91.7192, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(91.6015, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(90.9856, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(90.0517, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(89.8654, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(88.9584, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(88.8842, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(88.0921, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(86.8009, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(87.2464, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(86.5225, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(86.0828, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(85.6798, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(84.4367, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(85.1853, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(84.2475, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(83.8691, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(83.3658, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(82.9556, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(82.7379, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(82.3754, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(81.8543, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(81.3345, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(80.6543, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(80.7547, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(80.0977, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(80.1237, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(79.7469, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(79.2088, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(78.8810, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(78.7105, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(78.3467, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(78.1275, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(77.8875, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(77.5142, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(76.9495, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(77.1255, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(76.7300, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(76.2623, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(75.8344, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(75.8697, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(75.4865, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(75.3812, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(74.9531, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(74.5407, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(74.6322, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(74.4944, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(74.0171, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(73.7984, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(73.5726, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(73.3703, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(73.1319, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(72.3770, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "total_loss tensor(72.7360, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 181\n",
      "15001\n",
      "[INFO] Exp. name : cv11Lu_asr_lstm4atthead_allvocab-biclass2-5fold-2_sd0                                   \n",
      "[INFO] Loading data... large dataset may took a while.                                                     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of WhisperForAudioClassification were not initialized from the model checkpoint at sanchit-gandhi/whisper-large-v2-ft-ls-960h and are newly initialized: ['model.classifier.bias', 'model.classifier.weight', 'model.projector.bias', 'model.projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at google-bert/bert-base-chinese were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data for training/validation, store tokenizer and input/output shape\n",
      "Prepare dataloader for training/validation\n",
      "Interface for creating all kinds of dataset\n",
      "import VAGDataset as Dataset\n",
      "[VAGDataset] path: data_process, split: ['CTT5-2-15']\n",
      "Mozillacv11Dataset CTT5-2-15 found wav data: 41\n",
      "text len: 41\n",
      "remove None, then wav data: 41, text len: 41\n",
      "[VAGDataset] path: data_process, split: ['CTT5-1-15', 'CTT5-5-15', 'CTT5-3-15', 'CTT5-4-15']\n",
      "Mozillacv11Dataset CTT5-1-15 found wav data: 51\n",
      "Mozillacv11Dataset CTT5-5-15 found wav data: 45\n",
      "Mozillacv11Dataset CTT5-3-15 found wav data: 50\n",
      "Mozillacv11Dataset CTT5-4-15 found wav data: 45\n",
      "text len: 191\n",
      "remove None, then wav data: 191, text len: 191\n",
      "Setup ASR model and optimizer \n",
      "# Losses\n",
      "# Note: zero_infinity=False is unstable?\n",
      "# Optimizer\n",
      "[INFO] Optim.spec.| Algo. = Adadelta\t| Lr = 1.0\t (Scheduler = fixed)| Scheduled sampling = False           \n",
      "# Enable AMP if needed\n",
      "fc.weight\n",
      "fc.bias\n",
      "Training End-to-end ASR system\n",
      "[INFO] Total training steps 1.0K.                                                                          \n",
      "self.tr_set: 191\n",
      "total_loss tensor(129.2677, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(126.3532, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(126.7514, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(124.3900, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(123.1099, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(121.4377, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(119.9380, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(118.8437, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(117.2744, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(115.8784, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(114.5239, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(113.4000, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(111.8260, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(111.3036, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(109.8349, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(108.9366, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(107.9849, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(106.6136, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(105.8656, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(104.2894, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(103.8614, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(102.8990, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(102.0375, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(101.1451, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(100.5464, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(99.7832, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(98.3588, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(98.3934, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(97.4411, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(96.6212, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(96.1870, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(95.3025, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(94.4087, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(93.9821, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(93.4872, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(92.9663, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(92.1207, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(91.6951, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(91.1665, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(90.4295, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(90.0582, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(88.9734, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(89.0267, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(88.1614, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(87.7052, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(87.6649, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(86.8828, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(86.5038, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(86.1645, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(85.7240, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(84.8974, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(84.8971, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(84.6505, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(83.8174, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(83.7930, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(83.3524, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(82.8424, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(82.4077, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(82.1215, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(81.5442, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(81.2138, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(81.1289, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(80.7392, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(80.4512, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(80.0682, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(79.3792, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(79.4771, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(78.9869, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(78.8157, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(78.3448, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(78.0969, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(77.7631, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(77.5704, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(77.3575, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(77.1756, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(76.7244, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(76.4898, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "total_loss tensor(75.5133, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 191\n",
      "15001\n",
      "[INFO] Exp. name : cv11Lu_asr_lstm4atthead_allvocab-biclass2-5fold-3_sd0                                   \n",
      "[INFO] Loading data... large dataset may took a while.                                                     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of WhisperForAudioClassification were not initialized from the model checkpoint at sanchit-gandhi/whisper-large-v2-ft-ls-960h and are newly initialized: ['model.classifier.bias', 'model.classifier.weight', 'model.projector.bias', 'model.projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at google-bert/bert-base-chinese were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data for training/validation, store tokenizer and input/output shape\n",
      "Prepare dataloader for training/validation\n",
      "Interface for creating all kinds of dataset\n",
      "import VAGDataset as Dataset\n",
      "[VAGDataset] path: data_process, split: ['CTT5-3-15']\n",
      "Mozillacv11Dataset CTT5-3-15 found wav data: 50\n",
      "text len: 50\n",
      "remove None, then wav data: 50, text len: 50\n",
      "[VAGDataset] path: data_process, split: ['CTT5-1-15', 'CTT5-2-15', 'CTT5-5-15', 'CTT5-4-15']\n",
      "Mozillacv11Dataset CTT5-1-15 found wav data: 51\n",
      "Mozillacv11Dataset CTT5-2-15 found wav data: 41\n",
      "Mozillacv11Dataset CTT5-5-15 found wav data: 45\n",
      "Mozillacv11Dataset CTT5-4-15 found wav data: 45\n",
      "text len: 182\n",
      "remove None, then wav data: 182, text len: 182\n",
      "Setup ASR model and optimizer \n",
      "# Losses\n",
      "# Note: zero_infinity=False is unstable?\n",
      "# Optimizer\n",
      "[INFO] Optim.spec.| Algo. = Adadelta\t| Lr = 1.0\t (Scheduler = fixed)| Scheduled sampling = False           \n",
      "# Enable AMP if needed\n",
      "fc.weight\n",
      "fc.bias\n",
      "Training End-to-end ASR system\n",
      "[INFO] Total training steps 1.0K.                                                                          \n",
      "self.tr_set: 182\n",
      "total_loss tensor(125.2034, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(122.4484, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(120.8913, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(119.0108, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(118.7520, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(117.2662, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(116.2458, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(114.8913, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(113.9200, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(112.5975, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(111.3574, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(110.8339, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(109.8233, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(108.7940, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(107.5472, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(106.9956, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(106.0709, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(104.8719, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(104.6589, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(103.4101, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(103.0275, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(101.1966, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(101.5677, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(100.7195, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(99.6836, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(98.8325, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(98.7388, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(97.3799, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(97.1165, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(96.6243, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(95.8453, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(95.1046, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(93.8628, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(94.2713, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(93.4892, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(93.0002, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(92.4043, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(91.8878, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(91.2619, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(90.7461, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(89.9415, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(89.4295, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(89.3169, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(88.3148, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(87.6118, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(88.3071, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(87.5056, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(87.0237, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(86.5918, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(86.1489, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(85.3762, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(85.1985, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(84.2403, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(84.2975, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(84.1570, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(83.7982, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(83.1057, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(83.0818, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(82.2555, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(82.2827, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(82.3458, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(81.3200, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(80.7538, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(80.9220, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(80.5347, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(80.2591, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(79.8696, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(79.5742, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(79.2556, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(78.6982, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(78.9156, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(78.3259, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(77.6416, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(78.0174, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(77.4886, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(77.1594, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(76.5719, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(76.7132, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(75.5770, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(76.5319, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(75.9750, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "total_loss tensor(75.5100, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 182\n",
      "15001\n",
      "[INFO] Exp. name : cv11Lu_asr_lstm4atthead_allvocab-biclass2-5fold-4_sd0                                   \n",
      "[INFO] Loading data... large dataset may took a while.                                                     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of WhisperForAudioClassification were not initialized from the model checkpoint at sanchit-gandhi/whisper-large-v2-ft-ls-960h and are newly initialized: ['model.classifier.bias', 'model.classifier.weight', 'model.projector.bias', 'model.projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at google-bert/bert-base-chinese were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data for training/validation, store tokenizer and input/output shape\n",
      "Prepare dataloader for training/validation\n",
      "Interface for creating all kinds of dataset\n",
      "import VAGDataset as Dataset\n",
      "[VAGDataset] path: data_process, split: ['CTT5-4-15']\n",
      "Mozillacv11Dataset CTT5-4-15 found wav data: 45\n",
      "text len: 45\n",
      "remove None, then wav data: 45, text len: 45\n",
      "[VAGDataset] path: data_process, split: ['CTT5-1-15', 'CTT5-2-15', 'CTT5-3-15', 'CTT5-5-15']\n",
      "Mozillacv11Dataset CTT5-1-15 found wav data: 51\n",
      "Mozillacv11Dataset CTT5-2-15 found wav data: 41\n",
      "Mozillacv11Dataset CTT5-3-15 found wav data: 50\n",
      "Mozillacv11Dataset CTT5-5-15 found wav data: 45\n",
      "text len: 187\n",
      "remove None, then wav data: 187, text len: 187\n",
      "Setup ASR model and optimizer \n",
      "# Losses\n",
      "# Note: zero_infinity=False is unstable?\n",
      "# Optimizer\n",
      "[INFO] Optim.spec.| Algo. = Adadelta\t| Lr = 1.0\t (Scheduler = fixed)| Scheduled sampling = False           \n",
      "# Enable AMP if needed\n",
      "fc.weight\n",
      "fc.bias\n",
      "Training End-to-end ASR system\n",
      "[INFO] Total training steps 1.0K.                                                                          \n",
      "self.tr_set: 187\n",
      "total_loss tensor(129.1072, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(127.3658, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(126.2204, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(124.6172, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(122.4368, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(121.2641, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(120.3686, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(118.5956, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(117.2887, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(114.5264, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(115.1198, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(113.1156, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(111.3632, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(111.0222, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(108.7346, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(108.5907, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(107.4220, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(105.4754, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(104.4435, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(103.8416, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(103.4420, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(102.0654, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(100.5855, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(99.4386, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(99.4541, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(98.6396, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(97.6150, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(97.0314, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(96.0220, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(95.4118, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(93.9247, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(94.1530, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(93.2117, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(92.2299, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(91.9283, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(91.1672, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(90.3624, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(90.0535, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(89.7869, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(88.9131, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(88.2770, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(87.7614, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(87.1833, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(86.7983, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(85.8722, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(85.8870, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(85.2742, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(84.7793, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(83.9044, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(83.9503, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(83.0146, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(82.9928, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(82.5711, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(82.1533, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(81.5997, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(81.0601, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(81.1667, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(80.4691, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(79.9514, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(79.9636, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(79.5885, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(79.3157, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(78.8729, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(78.4853, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(78.1853, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(77.7822, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(77.4594, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(77.1015, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(77.0484, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(76.4460, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(76.4163, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(76.0078, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(76.0260, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(75.4630, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(75.2278, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(75.0655, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(74.4388, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(74.2389, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(74.4762, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(74.0381, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "15001\n",
      "[INFO] Exp. name : cv11Lu_asr_lstm4atthead_allvocab-biclass2-5fold-5_sd0                                   \n",
      "[INFO] Loading data... large dataset may took a while.                                                     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of WhisperForAudioClassification were not initialized from the model checkpoint at sanchit-gandhi/whisper-large-v2-ft-ls-960h and are newly initialized: ['model.classifier.bias', 'model.classifier.weight', 'model.projector.bias', 'model.projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at google-bert/bert-base-chinese were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data for training/validation, store tokenizer and input/output shape\n",
      "Prepare dataloader for training/validation\n",
      "Interface for creating all kinds of dataset\n",
      "import VAGDataset as Dataset\n",
      "[VAGDataset] path: data_process, split: ['CTT5-5-15']\n",
      "Mozillacv11Dataset CTT5-5-15 found wav data: 45\n",
      "text len: 45\n",
      "remove None, then wav data: 45, text len: 45\n",
      "[VAGDataset] path: data_process, split: ['CTT5-1-15', 'CTT5-2-15', 'CTT5-3-15', 'CTT5-4-15']\n",
      "Mozillacv11Dataset CTT5-1-15 found wav data: 51\n",
      "Mozillacv11Dataset CTT5-2-15 found wav data: 41\n",
      "Mozillacv11Dataset CTT5-3-15 found wav data: 50\n",
      "Mozillacv11Dataset CTT5-4-15 found wav data: 45\n",
      "text len: 187\n",
      "remove None, then wav data: 187, text len: 187\n",
      "Setup ASR model and optimizer \n",
      "# Losses\n",
      "# Note: zero_infinity=False is unstable?\n",
      "# Optimizer\n",
      "[INFO] Optim.spec.| Algo. = Adadelta\t| Lr = 1.0\t (Scheduler = fixed)| Scheduled sampling = False           \n",
      "# Enable AMP if needed\n",
      "fc.weight\n",
      "fc.bias\n",
      "Training End-to-end ASR system\n",
      "[INFO] Total training steps 1.0K.                                                                          \n",
      "self.tr_set: 187\n",
      "total_loss tensor(130.2917, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(127.4715, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(126.6394, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(123.9113, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(124.3742, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(121.6754, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(121.5951, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(119.8031, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(118.9588, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(117.5629, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(116.3531, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(115.6317, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(114.6532, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(113.7123, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(112.0519, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(112.0303, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(110.4275, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(110.3306, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(109.4771, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(108.5186, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(107.5651, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(106.9400, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(106.1364, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(105.4615, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(104.2245, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(104.1379, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(103.1764, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(102.1623, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(101.6047, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(101.8158, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(100.3431, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(99.8341, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(99.6602, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(99.0538, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(97.8147, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(97.0426, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(95.7967, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(97.9752, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(96.3190, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(95.7484, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(95.2833, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(94.8134, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(94.0542, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(94.0393, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(93.0475, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(92.6946, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(92.7720, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(91.9813, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(92.0631, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(91.2783, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(90.7612, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(90.5057, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(90.3366, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(89.5825, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(89.1320, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(88.8445, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(88.8386, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(88.2571, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(86.8870, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(88.3083, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(87.2682, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(86.4617, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(86.8379, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(86.3340, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(85.6945, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(85.9979, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(85.4495, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(85.2315, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(84.8904, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(84.2337, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(84.2969, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(84.2355, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(83.5794, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(83.5272, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(83.4531, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(82.9747, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(82.2194, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(82.7334, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(81.6503, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "total_loss tensor(82.3668, device='cuda:0', grad_fn=<AddBackward0>) \n",
      "\n",
      "self.tr_set: 187\n",
      "15001\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d455142",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
